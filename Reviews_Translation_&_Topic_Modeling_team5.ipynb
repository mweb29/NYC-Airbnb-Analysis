{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect, DetectorFactory\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting languages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating non-English comments: 100%|███████| 191/191 [00:34<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation complete. Only non-English texts were translated.\n"
     ]
    }
   ],
   "source": [
    "DetectorFactory.seed = 0\n",
    "\n",
    "API_KEY = \"\"\n",
    "TRANSLATE_URL = \"https://translation.googleapis.com/language/translate/v2\"\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text) if pd.notna(text) and text.strip() != \"\" else \"en\"\n",
    "    except:\n",
    "        return \"en\"\n",
    "\n",
    "def translate_batch(texts):\n",
    "    try:\n",
    "        if not texts:\n",
    "            return texts\n",
    "        \n",
    "        data = {\n",
    "            \"q\": texts,\n",
    "            \"target\": \"en\",\n",
    "            \"format\": \"text\"\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            TRANSLATE_URL,\n",
    "            params={\"key\": API_KEY},\n",
    "            json=data\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        return [t[\"translatedText\"] for t in result[\"data\"][\"translations\"]]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error translating batch: {e}\")\n",
    "        return texts\n",
    "\n",
    "reviews_data = pd.read_csv('reviews.csv')\n",
    "\n",
    "print(\"Detecting languages...\")\n",
    "reviews_data[\"lang\"] = reviews_data[\"comments\"].apply(detect_language)\n",
    "\n",
    "non_english_mask = reviews_data[\"lang\"] != \"en\"\n",
    "non_english_comments = reviews_data.loc[non_english_mask, \"comments\"].tolist()\n",
    "\n",
    "batch_size = 100\n",
    "translated_comments = []\n",
    "for i in tqdm(range(0, len(non_english_comments), batch_size), desc=\"Translating non-English comments\"):\n",
    "    batch = non_english_comments[i:i + batch_size]\n",
    "    translated_batch = translate_batch(batch)\n",
    "    translated_comments.extend(translated_batch)\n",
    "\n",
    "reviews_data.loc[non_english_mask, \"comments\"] = translated_comments\n",
    "\n",
    "reviews_data.to_csv('reviews_cleaned.csv', index=False)\n",
    "print(\"Translation complete. Only non-English texts were translated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"reviews_cleaned.csv\"\n",
    "reviews_data = pd.read_csv(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "      <th>word_count</th>\n",
       "      <th>has_html</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2595</td>\n",
       "      <td>17857</td>\n",
       "      <td>2009-11-21</td>\n",
       "      <td>50679</td>\n",
       "      <td>Jean</td>\n",
       "      <td>Our three-night stay. We enjoyed the apartment...</td>\n",
       "      <td>124</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2595</td>\n",
       "      <td>19176</td>\n",
       "      <td>2009-12-05</td>\n",
       "      <td>53267</td>\n",
       "      <td>Cate</td>\n",
       "      <td>Great experience.</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2595</td>\n",
       "      <td>19760</td>\n",
       "      <td>2009-12-10</td>\n",
       "      <td>38960</td>\n",
       "      <td>Anita</td>\n",
       "      <td>I've stayed with my friend at the Midtown Cast...</td>\n",
       "      <td>90</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2595</td>\n",
       "      <td>34320</td>\n",
       "      <td>2010-04-09</td>\n",
       "      <td>71130</td>\n",
       "      <td>Kai-Uwe</td>\n",
       "      <td>We've been staying here for about 9 nights, en...</td>\n",
       "      <td>66</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2595</td>\n",
       "      <td>46312</td>\n",
       "      <td>2010-05-25</td>\n",
       "      <td>117113</td>\n",
       "      <td>Alicia</td>\n",
       "      <td>We had a wonderful stay at Jennifer's charming...</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2595</td>\n",
       "      <td>1238204</td>\n",
       "      <td>2012-05-07</td>\n",
       "      <td>1783688</td>\n",
       "      <td>Sergey</td>\n",
       "      <td>Hi to everyone!\\rWould say our greatest compli...</td>\n",
       "      <td>99</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2595</td>\n",
       "      <td>1293632</td>\n",
       "      <td>2012-05-17</td>\n",
       "      <td>1870771</td>\n",
       "      <td>Loïc</td>\n",
       "      <td>Jennifer was very friendly and helpful, and he...</td>\n",
       "      <td>37</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2595</td>\n",
       "      <td>2022498</td>\n",
       "      <td>2012-08-18</td>\n",
       "      <td>2124102</td>\n",
       "      <td>Melanie</td>\n",
       "      <td>This apartment is like a real castle old and u...</td>\n",
       "      <td>208</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2595</td>\n",
       "      <td>4682989</td>\n",
       "      <td>2013-05-20</td>\n",
       "      <td>496053</td>\n",
       "      <td>Eric</td>\n",
       "      <td>Jennifer's place was in a great midtown locati...</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2595</td>\n",
       "      <td>13193832</td>\n",
       "      <td>2014-05-21</td>\n",
       "      <td>13685934</td>\n",
       "      <td>Gerald</td>\n",
       "      <td>Jennifer is a very nice host. Everything is cl...</td>\n",
       "      <td>25</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2595</td>\n",
       "      <td>15515108</td>\n",
       "      <td>2014-07-10</td>\n",
       "      <td>10781357</td>\n",
       "      <td>Richard</td>\n",
       "      <td>This is a cute studio in a wonderful location ...</td>\n",
       "      <td>51</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2595</td>\n",
       "      <td>20372242</td>\n",
       "      <td>2014-09-28</td>\n",
       "      <td>4212558</td>\n",
       "      <td>Carson</td>\n",
       "      <td>A great location, a very comfortable space and...</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2595</td>\n",
       "      <td>20937971</td>\n",
       "      <td>2014-10-07</td>\n",
       "      <td>13460520</td>\n",
       "      <td>Michael</td>\n",
       "      <td>Jennifer's place is cozy and a short walking d...</td>\n",
       "      <td>39</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2595</td>\n",
       "      <td>21464102</td>\n",
       "      <td>2014-10-18</td>\n",
       "      <td>16584002</td>\n",
       "      <td>Kitty</td>\n",
       "      <td>We enjoyed our stay in Jennifer's apartment. E...</td>\n",
       "      <td>36</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2595</td>\n",
       "      <td>28794060</td>\n",
       "      <td>2015-03-30</td>\n",
       "      <td>27436102</td>\n",
       "      <td>Kellie</td>\n",
       "      <td>Jennifer was very good at communicating with u...</td>\n",
       "      <td>146</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2595</td>\n",
       "      <td>30430122</td>\n",
       "      <td>2015-04-21</td>\n",
       "      <td>6429364</td>\n",
       "      <td>Sonya</td>\n",
       "      <td>I love this space.  It is truly a gem in the h...</td>\n",
       "      <td>34</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2595</td>\n",
       "      <td>32532759</td>\n",
       "      <td>2015-05-19</td>\n",
       "      <td>12146524</td>\n",
       "      <td>Michiel</td>\n",
       "      <td>This was our first Airbnb experience, and Jenn...</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2595</td>\n",
       "      <td>47785650</td>\n",
       "      <td>2015-09-21</td>\n",
       "      <td>12885233</td>\n",
       "      <td>Lin</td>\n",
       "      <td>Great location, in the middle of everything. T...</td>\n",
       "      <td>26</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2595</td>\n",
       "      <td>48764464</td>\n",
       "      <td>2015-09-28</td>\n",
       "      <td>18636130</td>\n",
       "      <td>Alex</td>\n",
       "      <td>Jennifer was without doubt an amazing host, sh...</td>\n",
       "      <td>92</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2595</td>\n",
       "      <td>69547296</td>\n",
       "      <td>2016-04-11</td>\n",
       "      <td>16512817</td>\n",
       "      <td>Gary</td>\n",
       "      <td>Jennifer was an unbelievable host! Great commu...</td>\n",
       "      <td>112</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    listing_id        id        date  reviewer_id reviewer_name  \\\n",
       "0         2595     17857  2009-11-21        50679          Jean   \n",
       "1         2595     19176  2009-12-05        53267          Cate   \n",
       "2         2595     19760  2009-12-10        38960         Anita   \n",
       "3         2595     34320  2010-04-09        71130       Kai-Uwe   \n",
       "4         2595     46312  2010-05-25       117113        Alicia   \n",
       "5         2595   1238204  2012-05-07      1783688        Sergey   \n",
       "6         2595   1293632  2012-05-17      1870771          Loïc   \n",
       "7         2595   2022498  2012-08-18      2124102       Melanie   \n",
       "8         2595   4682989  2013-05-20       496053          Eric   \n",
       "9         2595  13193832  2014-05-21     13685934        Gerald   \n",
       "10        2595  15515108  2014-07-10     10781357       Richard   \n",
       "11        2595  20372242  2014-09-28      4212558        Carson   \n",
       "12        2595  20937971  2014-10-07     13460520       Michael   \n",
       "13        2595  21464102  2014-10-18     16584002         Kitty   \n",
       "14        2595  28794060  2015-03-30     27436102        Kellie   \n",
       "15        2595  30430122  2015-04-21      6429364         Sonya   \n",
       "16        2595  32532759  2015-05-19     12146524       Michiel   \n",
       "17        2595  47785650  2015-09-21     12885233           Lin   \n",
       "18        2595  48764464  2015-09-28     18636130          Alex   \n",
       "19        2595  69547296  2016-04-11     16512817          Gary   \n",
       "\n",
       "                                             comments  word_count  has_html  \n",
       "0   Our three-night stay. We enjoyed the apartment...         124     False  \n",
       "1                                   Great experience.           2     False  \n",
       "2   I've stayed with my friend at the Midtown Cast...          90     False  \n",
       "3   We've been staying here for about 9 nights, en...          66     False  \n",
       "4   We had a wonderful stay at Jennifer's charming...          24     False  \n",
       "5   Hi to everyone!\\rWould say our greatest compli...          99     False  \n",
       "6   Jennifer was very friendly and helpful, and he...          37     False  \n",
       "7   This apartment is like a real castle old and u...         208     False  \n",
       "8   Jennifer's place was in a great midtown locati...          57     False  \n",
       "9   Jennifer is a very nice host. Everything is cl...          25     False  \n",
       "10  This is a cute studio in a wonderful location ...          51     False  \n",
       "11  A great location, a very comfortable space and...          16     False  \n",
       "12  Jennifer's place is cozy and a short walking d...          39     False  \n",
       "13  We enjoyed our stay in Jennifer's apartment. E...          36     False  \n",
       "14  Jennifer was very good at communicating with u...         146     False  \n",
       "15  I love this space.  It is truly a gem in the h...          34     False  \n",
       "16  This was our first Airbnb experience, and Jenn...          32     False  \n",
       "17  Great location, in the middle of everything. T...          26     False  \n",
       "18  Jennifer was without doubt an amazing host, sh...          92     False  \n",
       "19  Jennifer was an unbelievable host! Great commu...         112     False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data First Few Lines\n",
    "reviews_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 969486 entries, 0 to 969485\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype         \n",
      "---  ------         --------------   -----         \n",
      " 0   listing_id     969486 non-null  int64         \n",
      " 1   id             969486 non-null  int64         \n",
      " 2   date           969486 non-null  datetime64[ns]\n",
      " 3   reviewer_id    969486 non-null  int64         \n",
      " 4   reviewer_name  969486 non-null  object        \n",
      " 5   comments       969485 non-null  object        \n",
      " 6   word_count     969486 non-null  int64         \n",
      " 7   has_html       969486 non-null  bool          \n",
      "dtypes: bool(1), datetime64[ns](1), int64(4), object(2)\n",
      "memory usage: 52.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#Change Date Type\n",
    "reviews_data['date'] = pd.to_datetime(reviews_data['date'])\n",
    "\n",
    "reviews_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.694860e+05</td>\n",
       "      <td>9.694860e+05</td>\n",
       "      <td>969486</td>\n",
       "      <td>9.694860e+05</td>\n",
       "      <td>969486.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.636331e+17</td>\n",
       "      <td>4.890135e+17</td>\n",
       "      <td>2021-02-20 06:09:18.466651648</td>\n",
       "      <td>1.612247e+08</td>\n",
       "      <td>44.981082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.595000e+03</td>\n",
       "      <td>3.149000e+03</td>\n",
       "      <td>2009-05-25 00:00:00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.841695e+06</td>\n",
       "      <td>3.664798e+08</td>\n",
       "      <td>2019-01-02 00:00:00</td>\n",
       "      <td>3.144509e+07</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.761288e+07</td>\n",
       "      <td>5.209964e+17</td>\n",
       "      <td>2021-12-19 00:00:00</td>\n",
       "      <td>1.058853e+08</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.170911e+07</td>\n",
       "      <td>9.101962e+17</td>\n",
       "      <td>2023-06-09 00:00:00</td>\n",
       "      <td>2.506889e+08</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.308179e+18</td>\n",
       "      <td>1.325553e+18</td>\n",
       "      <td>2025-01-02 00:00:00</td>\n",
       "      <td>6.696213e+08</td>\n",
       "      <td>1001.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.350861e+17</td>\n",
       "      <td>4.734849e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.572517e+08</td>\n",
       "      <td>45.958727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         listing_id            id                           date  \\\n",
       "count  9.694860e+05  9.694860e+05                         969486   \n",
       "mean   1.636331e+17  4.890135e+17  2021-02-20 06:09:18.466651648   \n",
       "min    2.595000e+03  3.149000e+03            2009-05-25 00:00:00   \n",
       "25%    9.841695e+06  3.664798e+08            2019-01-02 00:00:00   \n",
       "50%    2.761288e+07  5.209964e+17            2021-12-19 00:00:00   \n",
       "75%    5.170911e+07  9.101962e+17            2023-06-09 00:00:00   \n",
       "max    1.308179e+18  1.325553e+18            2025-01-02 00:00:00   \n",
       "std    3.350861e+17  4.734849e+17                            NaN   \n",
       "\n",
       "        reviewer_id     word_count  \n",
       "count  9.694860e+05  969486.000000  \n",
       "mean   1.612247e+08      44.981082  \n",
       "min    1.000000e+00       1.000000  \n",
       "25%    3.144509e+07      15.000000  \n",
       "50%    1.058853e+08      32.000000  \n",
       "75%    2.506889e+08      60.000000  \n",
       "max    6.696213e+08    1001.000000  \n",
       "std    1.572517e+08      45.958727  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Description\n",
    "reviews_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "listing_id       969486\n",
       "id               969486\n",
       "date             969486\n",
       "reviewer_id      969486\n",
       "reviewer_name    969486\n",
       "comments         969485\n",
       "word_count       969486\n",
       "has_html         969486\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total values per column\n",
    "reviews_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/6fgg1z7510q7jrn5jmwz63tm0000gn/T/ipykernel_92070/2331579940.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  reviews_data['comments'].fillna(\"blank\", inplace=True)\n",
      "/var/folders/ty/6fgg1z7510q7jrn5jmwz63tm0000gn/T/ipykernel_92070/2331579940.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  reviews_data['reviewer_name'].fillna(\"none\", inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "listing_id       0\n",
       "id               0\n",
       "date             0\n",
       "reviewer_id      0\n",
       "reviewer_name    0\n",
       "comments         0\n",
       "word_count       0\n",
       "has_html         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_data['comments'].fillna(\"blank\", inplace=True)\n",
    "reviews_data['reviewer_name'].fillna(\"none\", inplace=True)\n",
    "reviews_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Cleaning & Preprocessing for Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/var/folders/ty/6fgg1z7510q7jrn5jmwz63tm0000gn/T/ipykernel_92070/1734315462.py:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  reviews = pd.DataFrame(reviews.comments.str.replace('[^\\w\\s]','', regex=True)) # remove punctuation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>our threenight stay we enjoyed the apartment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ive stayed with my friend at the midtown castl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weve been staying here for about 9 nights enjo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we had a wonderful stay at jennifers charming ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments\n",
       "0  our threenight stay we enjoyed the apartment w...\n",
       "1                                   great experience\n",
       "2  ive stayed with my friend at the midtown castl...\n",
       "3  weve been staying here for about 9 nights enjo...\n",
       "4  we had a wonderful stay at jennifers charming ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.DataFrame(reviews_data.comments.str.lower()) #make everything lowercase\n",
    "reviews = pd.DataFrame(reviews.comments.str.replace('[^\\w\\s]','', regex=True)) # remove punctuation\n",
    "reviews = pd.DataFrame(reviews.comments.str.strip()) # removing leading & trailing spaces\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Total Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments for topic modeling: 100%|█| 969486/969486 [00:00<00:00, 8489\n",
      "Fitting BERTopic model:   0%|                        | 0/969486 [00:00<?, ?it/s]2025-02-28 17:16:59,670 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61549476e3a649d389b9b8226b8c5b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/30297 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 17:30:03,119 - BERTopic - Embedding - Completed ✓\n",
      "2025-02-28 17:30:03,120 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-02-28 18:57:04,162 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-02-28 18:57:04,217 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-02-28 18:58:29,688 - BERTopic - Cluster - Completed ✓\n",
      "2025-02-28 18:58:29,809 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-02-28 18:59:04,120 - BERTopic - Representation - Completed ✓\n",
      "Fitting BERTopic model: 100%|████████| 969486/969486 [1:42:40<00:00, 157.37it/s]\n",
      "2025-02-28 18:59:40,228 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-02-28 19:00:13,139 - BERTopic - Topic reduction - Reduced number of topics from 4861 to 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topic   Count                                               Name  \\\n",
      "0      -1  374768                          -1_great_stay_place_clean   \n",
      "1       0  570453                       0_great_place_stay_apartment   \n",
      "2       1   15482                       1_good_great_thank_good good   \n",
      "3       2    3322                                 2_na_yes_na na_ras   \n",
      "4       3    1568    3_recommended_recommend_highly_recommend highly   \n",
      "5       4     975               4_value_great value_good value_money   \n",
      "6       5     720                            5_ok_ok ok_okay_alright   \n",
      "7       6     669            6_advertised_described_expected_exactly   \n",
      "8       7     397          7_blank_blank blank_comment_comment blank   \n",
      "9       8     283                      8_review_thumbs_previous_time   \n",
      "10      9     258     9_1010_1010 1010_recommend 1010_1010 recommend   \n",
      "11     10     163    10_needed_exactly_exactly needed_needed exactly   \n",
      "12     11     116            11_wont_regret_wont regret_disappointed   \n",
      "13     12      81                      12_water_hot_hot water_shower   \n",
      "14     13      66              13_terrible_horrible_worst_experience   \n",
      "15     14      59                                   14_et_la_trs_und   \n",
      "16     15      58                 15_close close_close_close jfk_jfk   \n",
      "17     16      20       16_genial_genial genial_henial genial_henial   \n",
      "18     17      16  17_optimal optimal_optimal_surrounding choices...   \n",
      "19     18      12  18_host accommodating_accommodating_accommodat...   \n",
      "\n",
      "                                       Representation  \\\n",
      "0   [great, stay, place, clean, location, apartmen...   \n",
      "1   [great, place, stay, apartment, clean, locatio...   \n",
      "2   [good, great, thank, good good, service, excel...   \n",
      "3   [na, yes, na na, ras, a1, na yes, meh, yes na,...   \n",
      "4   [recommended, recommend, highly, recommend hig...   \n",
      "5   [value, great value, good value, money, price,...   \n",
      "6   [ok, ok ok, okay, alright, okay ok, ok okay, a...   \n",
      "7   [advertised, described, expected, exactly, bet...   \n",
      "8   [blank, blank blank, comment, comment blank, b...   \n",
      "9   [review, thumbs, previous, time, write, gladly...   \n",
      "10  [1010, 1010 1010, recommend 1010, 1010 recomme...   \n",
      "11  [needed, exactly, exactly needed, needed exact...   \n",
      "12  [wont, regret, wont regret, disappointed, wont...   \n",
      "13  [water, hot, hot water, shower, water shower, ...   \n",
      "14  [terrible, horrible, worst, experience, better...   \n",
      "15  [et, la, trs, und, en, ted, nous, est, es, jenny]   \n",
      "16  [close close, close, close jfk, jfk, jfk close...   \n",
      "17  [genial, genial genial, henial genial, henial,...   \n",
      "18  [optimal optimal, optimal, surrounding choices...   \n",
      "19  [host accommodating, accommodating, accommodat...   \n",
      "\n",
      "                                  Representative_Docs  \n",
      "0   [A very nice place to stay, I would stay again...  \n",
      "1   [Great location. Good place to stay. Clean., G...  \n",
      "2               [good, It was good, It was very good]  \n",
      "3                           [Yes, they did, Yes, Yes]  \n",
      "4   [Highly recommended!, Highly recommended, High...  \n",
      "5   [Good value for money, Good value for money., ...  \n",
      "6                                        [Ok, Ok, Ok]  \n",
      "7   [Exactly as advertised, Exactly as advertised!...  \n",
      "8                               [blank, blank, blank]  \n",
      "9                   [No review, no review, No review]  \n",
      "10  [10/10, Would recommend 10/10, Would recommend...  \n",
      "11  [It was exactly what we needed!, Exactly what ...  \n",
      "12  [You will not regret., You never regret, u won...  \n",
      "13  [No hot water, No hot water in the shower!!!!,...  \n",
      "14                     [Terrible, terrible, Terrible]  \n",
      "15  [Dès le premier jour l'accueil était chaleureu...  \n",
      "16  [Very close to JFK, Very close to JFK,, Close ...  \n",
      "17                       [Genial!!!, Genial, Genial.]  \n",
      "18                        [Optimal, Optimal, Optimal]  \n",
      "19  [Host was very accommodating and kind, the hos...  \n"
     ]
    }
   ],
   "source": [
    "comments = reviews_data['comments'].dropna().tolist()\n",
    "\n",
    "comments = list(tqdm(comments, desc=\"Processing comments for topic modeling\"))\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    language=\"english\",\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    nr_topics=None, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "with tqdm(total=len(comments), desc=\"Fitting BERTopic model\") as pbar:\n",
    "    topics, probabilities = topic_model.fit_transform(comments)\n",
    "    pbar.update(len(comments))\n",
    "\n",
    "topic_model.reduce_topics(comments)\n",
    "\n",
    "print(topic_model.get_topic_info())\n",
    "topic_info = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info.to_csv('total_topic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Post Dataset (Post, Sep/05/2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_data['date'] = pd.to_datetime(reviews_data['date'], errors='coerce')\n",
    "df_post = reviews_data[reviews_data['date'] >= '2023-09-05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting topic modeling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   0%|                          | 0/185779 [00:00<?, ?it/s]2025-03-01 15:21:30,401 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c8a843eb454630960beb07380c3804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5806 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 15:23:50,583 - BERTopic - Embedding - Completed ✓\n",
      "2025-03-01 15:23:50,583 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-03-01 15:24:45,282 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-03-01 15:24:45,289 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-03-01 15:24:50,367 - BERTopic - Cluster - Completed ✓\n",
      "2025-03-01 15:24:50,368 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-03-01 15:25:22,433 - BERTopic - Representation - Completed ✓\n",
      "2025-03-01 15:25:22,531 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-03-01 15:25:43,040 - BERTopic - Topic reduction - Reduced number of topics from 192 to 10\n",
      "Processing documents: 100%|████████████| 185779/185779 [04:12<00:00, 734.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reassigning 82506 outliers to the closest cluster...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d47d6b7c15409dbfed9e7c7def69e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5806 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 15:29:13,281 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic info with meaningful representations and sentences (excluding Topic -1):\n",
      "   Topic   Count                                  Name  \\\n",
      "0      0  166022                      0_the_and_to_was   \n",
      "1      1    3596           1_good_great_nice_excellent   \n",
      "2      2    7027                      2_the_was_to_and   \n",
      "3      3    4029          3_everything_was_thank_great   \n",
      "4      4    2225                  4_clean_and_very_was   \n",
      "5      5    1192       5_experience_service_great_very   \n",
      "6      6     744                6_value_price_for_good   \n",
      "7      7     439  7_recommend_recommended_highly_would   \n",
      "8      8     505           8_as_exactly_what_described   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [the, and, to, was, is, in, very, stay, for, g...   \n",
      "1  [good, great, nice, excellent, perfect, thank,...   \n",
      "2     [the, was, to, and, not, in, it, of, that, we]   \n",
      "3  [everything, was, thank, great, you, it, perfe...   \n",
      "4  [clean, and, very, was, the, everything, nice,...   \n",
      "5  [experience, service, great, very, good, was, ...   \n",
      "6  [value, price, for, good, money, worth, the, g...   \n",
      "7  [recommend, recommended, highly, would, it, 10...   \n",
      "8  [as, exactly, what, described, everything, adv...   \n",
      "\n",
      "                             Representative_Sentence  \n",
      "0                                      A great stay!  \n",
      "1                                          Very good  \n",
      "2  Other reviews mentioned unadvertised fees, so ...  \n",
      "3                              Perfect as always 💯💯💯  \n",
      "4                                         Very clean  \n",
      "5                           I had a great experience  \n",
      "6                              Good value for money.  \n",
      "7                                 I highly recommend  \n",
      "8         everything was exactly as it was described  \n"
     ]
    }
   ],
   "source": [
    "docs = df_post['comments'].tolist()\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3)\n",
    ")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=150,\n",
    "    min_samples=10,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom'\n",
    ")\n",
    "\n",
    "representation_model = {\n",
    "    \"Main\": KeyBERTInspired(),\n",
    "    \"MMR\": MaximalMarginalRelevance(diversity=0.3)\n",
    "}\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    vectorizer_model=vectorizer,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    representation_model=representation_model,\n",
    "    nr_topics=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Starting topic modeling...\")\n",
    "with tqdm(total=len(docs), desc=\"Processing documents\") as pbarzzzzzzz\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "    pbar.update(len(docs))\n",
    "\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "outlier_indices = [i for i, t in enumerate(topics) if t == -1]\n",
    "if outlier_indices:\n",
    "    print(f\"Reassigning {len(outlier_indices)} outliers to the closest cluster...\")\n",
    "    \n",
    "    embeddings = topic_model.embedding_model.embedding_model.encode(docs, show_progress_bar=True)\n",
    "    \n",
    "    topic_embeddings = {}\n",
    "    for topic_id in topic_info['Topic']:\n",
    "        if topic_id != -1:\n",
    "            topic_docs = [embeddings[i] for i, t in enumerate(topics) if t == topic_id]\n",
    "            topic_embeddings[topic_id] = np.mean(topic_docs, axis=0)\n",
    "    \n",
    "    for i in outlier_indices:\n",
    "        doc_embedding = embeddings[i].reshape(1, -1)\n",
    "        similarities = {t: cosine_similarity(doc_embedding, emb.reshape(1, -1))[0, 0] for t, emb in topic_embeddings.items()}\n",
    "        closest_topic = max(similarities, key=similarities.get)\n",
    "        topics[i] = closest_topic\n",
    "    topic_model.update_topics(docs, topics=topics)\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "representative_docs = {}\n",
    "for topic in topic_info['Topic']:\n",
    "    reps = topic_model.get_representative_docs(topic)\n",
    "    representative_docs[topic] = reps[0] if reps else \"No representative document\"\n",
    "\n",
    "topic_info['Representative_Sentence'] = topic_info['Topic'].map(representative_docs)\n",
    "\n",
    "print(\"Topic info with meaningful representations and sentences (excluding Topic -1):\")\n",
    "print(topic_info[['Topic', 'Count', 'Name', 'Representation', 'Representative_Sentence']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info.to_csv('post_topic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre Dataset (Pre, Sep/05/2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_data['date'] = pd.to_datetime(reviews_data['date'], errors='coerce')\n",
    "df_pre = reviews_data[reviews_data['date'] < '2023-09-05']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting topic modeling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   0%|                          | 0/781039 [00:00<?, ?it/s]2025-03-02 01:00:24,291 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b04e274d374f1fadf96e860c046aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/24408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 01:12:21,868 - BERTopic - Embedding - Completed ✓\n",
      "2025-03-02 01:12:21,869 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-03-02 01:41:58,162 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-03-02 01:41:58,188 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-03-02 01:42:44,551 - BERTopic - Cluster - Completed ✓\n",
      "2025-03-02 01:42:44,552 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-03-02 01:46:07,579 - BERTopic - Representation - Completed ✓\n",
      "2025-03-02 01:46:07,953 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-03-02 01:47:31,727 - BERTopic - Topic reduction - Reduced number of topics from 842 to 10\n",
      "Processing documents: 100%|████████████| 781039/781039 [47:07<00:00, 276.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reassigning 279574 outliers to the closest cluster...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae75be90fa3472699d00f972b0f76c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/24408 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 02:07:55,122 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic info with meaningful representations and sentences (excluding Topic -1):\n",
      "   Topic   Count                          Name  \\\n",
      "0      0  732215              0_the_and_to_was   \n",
      "1      1   22492   1_great_was_everything_very   \n",
      "2      2    7789   2_good_great_excellent_nice   \n",
      "3      3   13329  3_clean_very_and_comfortable   \n",
      "4      4    2454      4_value_price_good_money   \n",
      "5      5     955            5_ok_all_good_okay   \n",
      "6      6     472           6_blank_top_no_none   \n",
      "7      7     439        7_stars_star_five_host   \n",
      "8      8     894        8_review_as_no_comment   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [the, and, to, was, is, in, very, for, we, great]   \n",
      "1  [great, was, everything, very, and, host, than...   \n",
      "2  [good, great, excellent, nice, amazing, very, ...   \n",
      "3  [clean, very, and, comfortable, was, nice, the...   \n",
      "4  [value, price, good, money, for, great, worth,...   \n",
      "5  [ok, all, good, okay, thanks, alright, everyth...   \n",
      "6  [blank, top, no, none, nothing, tv, the, solid...   \n",
      "7  [stars, star, five, host, great, the, all, giv...   \n",
      "8  [review, as, no, comment, not, described, agai...   \n",
      "\n",
      "       Representative_Sentence  \n",
      "0                  Great place  \n",
      "1  it was a great experience.   \n",
      "2                         good  \n",
      "3                   It’s clean  \n",
      "4   Very good value for money!  \n",
      "5                           Ok  \n",
      "6                        blank  \n",
      "7                  Five stars!  \n",
      "8                       review  \n"
     ]
    }
   ],
   "source": [
    "docs = df_pre['comments'].tolist()\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3)\n",
    ")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=150,\n",
    "    min_samples=10,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom'\n",
    ")\n",
    "\n",
    "representation_model = {\n",
    "    \"Main\": KeyBERTInspired(),\n",
    "    \"MMR\": MaximalMarginalRelevance(diversity=0.3)\n",
    "}\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    vectorizer_model=vectorizer,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    representation_model=representation_model,\n",
    "    nr_topics=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Starting topic modeling...\")\n",
    "with tqdm(total=len(docs), desc=\"Processing documents\") as pbar:\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "    pbar.update(len(docs))\n",
    "\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "outlier_indices = [i for i, t in enumerate(topics) if t == -1]\n",
    "if outlier_indices:\n",
    "    print(f\"Reassigning {len(outlier_indices)} outliers to the closest cluster...\")\n",
    "    \n",
    "    embeddings = topic_model.embedding_model.embedding_model.encode(docs, show_progress_bar=True)\n",
    "    \n",
    "    topic_embeddings = {}\n",
    "    for topic_id in topic_info['Topic']:\n",
    "        if topic_id != -1:\n",
    "            topic_docs = [embeddings[i] for i, t in enumerate(topics) if t == topic_id]\n",
    "            topic_embeddings[topic_id] = np.mean(topic_docs, axis=0)\n",
    "    \n",
    "    for i in outlier_indices:\n",
    "        doc_embedding = embeddings[i].reshape(1, -1)\n",
    "        similarities = {t: cosine_similarity(doc_embedding, emb.reshape(1, -1))[0, 0] for t, emb in topic_embeddings.items()}\n",
    "        closest_topic = max(similarities, key=similarities.get)\n",
    "        topics[i] = closest_topic\n",
    "    topic_model.update_topics(docs, topics=topics)\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "representative_docs = {}\n",
    "for topic in topic_info['Topic']:\n",
    "    reps = topic_model.get_representative_docs(topic)\n",
    "    representative_docs[topic] = reps[0] if reps else \"No representative document\"\n",
    "\n",
    "topic_info['Representative_Sentence'] = topic_info['Topic'].map(representative_docs)\n",
    "\n",
    "print(\"Topic info with meaningful representations and sentences (excluding Topic -1):\")\n",
    "print(topic_info[['Topic', 'Count', 'Name', 'Representation', 'Representative_Sentence']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info.to_csv('pre_topic.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
